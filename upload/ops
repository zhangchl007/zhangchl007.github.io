1.desktop 10.56.87.8

fr-internal.huawei.com
apps.mtl-expcloud.huawei.com

1. generated ssh-keys

ssh-keygen -t rsa -f /root/.ssh/id_rsa -P ''
 for i in `awk '{print $1}' /root/huawei/ocp/nodelist `;do ssh-copy-id -i ~/.ssh/id_rsa.pub $i;done
 

3.2.2 配置DNS服务器
3.2.2.1 安装DNS服务
与DNS服务器上安装DNS服务软件包BIND。执行如下命令：
yum -y install bind iptables-services;
systemctl enable named ;
3.2.2.2 修改配置
修改配置文件/etc/named.conf。命令如下：
cp /etc/named.conf /etc/named.conf.bak.$(date "+%Y%m%d%H%M%S");
sed -i s/"listen-on port 53 { 127.0.0.1; };"/"listen-on port 53 { any; };"/g /etc/named.conf;
sed -i s/"listen-on-v6 port 53 { ::1; };"/"listen-on port 53 { any; };"/g /etc/named.conf;
sed -i s/"allow-query     { localhost; };"/"allow-query     { any; };"/g /etc/named.conf;
sed -i '/rfc1912/i\zone "apps.mtl-expcloud.huawei.com" IN { type master; file "dynamic/apps.mtl-expcloud.huawei.com.db"; };' /etc/named.conf
sed -i '/rfc1912/i\zone "fr-internal.huawei.com" IN { type master; file "dynamic/fr-internal.huawei.com.db"; };' /etc/named.conf		

3.2.2.3 创建域名数据库文件
在DNS服务器上创建域名数据库文件。需要注意subdomain的配置，本示例用的apps.example.com,需要后面定义ansible inventory的subdomain一致.命令如下：
文件1： 
cat << EOF > /var/named/dynamic/fr-internal.huawei.com.db
\$ORIGIN .
\$TTL 1 ; 1 sec
fr-internal.huawei.com     IN  SOA  ns1.fr-internal.huawei.com. hostmaster.fr-internal.huawei.com. (
                2011112904 ; serial
                60         ; refresh
                15         ; retry
                1800       ; expire
                10         ; minimum
           )
       NS ns1.fr-internal.huawei.com.
       MX 10 mail.fr-internal.huawei.com.
\$ORIGIN fr-internal.huawei.com.
ns1 A 10.206.98.140
registry A 10.206.98.209
ldap A 10.206.98.209
openshift-cluster A 10.206.98.122
yum A 10.206.98.209
infra-svc01 10.206.98.140
infra-reg01 10.206.98.103
infra-reg02 10.206.98.106
ldap01 10.206.98.72
ldap02 10.206.98.109
infra-lb01 10.206.98.154
infra-lb02 10.206.98.190
node01 10.221.177.75
node02 10.221.177.146
node03 10.206.99.164
node04 10.221.177.209
infra-node01 10.221.177.216
infra-node02 10.221.177.226
master02 10.206.96.164
master01 10.206.97.100
master03 10.221.181.201
EOF

文件2：


cat << EOF > /var/named/dynamic/apps.mtl-expcloud.huawei.com.db
\$ORIGIN .
\$TTL 1 ; 1 sec
apps.mtl-expcloud.huawei.com     IN  SOA  ns1.apps.mtl-expcloud.huawei.com. hostmaster.apps.mtl-expcloud.huawei.com. (
              2011112904 ; serial
              60         ; refresh
              15         ; retry
              1800       ; expire
              10         ; minimum
           )
       NS ns1.apps.mtl-expcloud.huawei.com.
       MX 10 mail.apps.mtl-expcloud.huawei.com.
\$ORIGIN apps.mtl-expcloud.huawei.com.
*        A 10.206.98.209
EOF
  

3.2.2.4 重启BIND服务
重启BIND服务，使修改的配置生效。
systemctl restart named;
3.2.2.5 配置iptables

修改iptables规则。命令如下：
cp /etc/sysconfig/iptables /etc/sysconfig/iptables.bak.$(date "+%Y%m%d%H%M%S");
sed -i '/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 53 -j ACCEPT' /etc/sysconfig/iptables;
sed -i '/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p udp -m state --state NEW -m udp --dport 53 -j ACCEPT' /etc/sysconfig/iptables;
sed -i '/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 5000 -j ACCEPT' /etc/sysconfig/iptables;
sed -i '/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 80 -j ACCEPT' /etc/sysconfig/iptables;

systemctl restart iptables;
3.2.2.6 配置DNS解析
在所有的节点上配置启用新搭建的DNS服务器进行域名解析。命令如下：

nmcli conn modify eth0 ipv4.dns 10.206.98.140

3.2.2.7 测试DNS解析
nslookup test01.mtl-expcloud.huawei.com
master01.fr-internal.huawei.com

Openldap installation step

yum -y install openldap-servers openldap-clients
cp /usr/share/openldap-servers/DB_CONFIG.example /var/lib/ldap/DB_CONFIG
chown ldap. /var/lib/ldap/DB_CONFIG 
systemctl start slapd && systemctl enable slapd


slappasswd
admin
{SSHA}nj9Lr18Vf7DeKIbazVxOXcgw+RRDuxZv

cat << EOF > chrootpw.ldif 
# specify the password generated above for "olcRootPW" section

dn: olcDatabase={0}config,cn=config
changetype: modify
add: olcRootPW
olcRootPW: {SSHA}47Ij/IKdYB9LS29Rko4WitDgZrUJ9zra
EOF

ldapadd -Y EXTERNAL -H ldapi:/// -f chrootpw.ldif

ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/cosine.ldif
ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/nis.ldif 
ldapadd -Y EXTERNAL -H ldapi:/// -f /etc/openldap/schema/inetorgperson.ldif 


cat << EOF > chdomain.ldif
# replace to your own domain name for "dc=***,dc=***" section
# specify the password generated above for "olcRootPW" section

dn: olcDatabase={1}monitor,cn=config
changetype: modify
replace: olcAccess
olcAccess: {0}to * by dn.base="gidNumber=0+uidNumber=0,cn=peercred,cn=external,cn=auth"
  read by dn.base="cn=Manager,dc=fr-internal,dc=huawei,dc=com" read by * none

dn: olcDatabase={2}hdb,cn=config
changetype: modify
replace: olcSuffix
olcSuffix: dc=fr-internal,dc=huawei,dc=com

dn: olcDatabase={2}hdb,cn=config
changetype: modify
replace: olcRootDN
olcRootDN: cn=Manager,dc=fr-internal,dc=huawei,dc=com

dn: olcDatabase={2}hdb,cn=config
changetype: modify
add: olcRootPW
olcRootPW: {SSHA}cYYZs401mWwggn5cXlYMvcsYnYLvF1Ts

dn: olcDatabase={2}hdb,cn=config
changetype: modify
add: olcAccess
olcAccess: {0}to attrs=userPassword,shadowLastChange by
  dn="cn=Manager,dc=fr-internal,dc=huawei,dc=com" write by anonymous auth by self write by * none
olcAccess: {1}to dn.base="" by * read
olcAccess: {2}to * by dn="cn=Manager,dc=fr-internal,dc=huawei,dc=com" write by * read
EOF

ldapmodify -Y EXTERNAL -H ldapi:/// -f chdomain.ldif 


yum install php php-ldap webserver -y 
rpm -ivh phpldapadmin-1.2.3-10.el7.noarch.rpm  


 vi /etc/httpd/conf.d/phpldapadmin.conf
 
  Require all granted
  
systemctl restart httpd.service
systemctl enable httpd.service

vi /etc/phpldapadmin/config.php

Uncomment the line 397 and comment out the 398, like below.

$servers->setValue('login','attr','dn');
// $servers->setValue('login','attr','uid');



ldap01 node

cat << EOF > /etc/openldap/slapd.conf
include     /etc/openldap/schema/core.schema
include     /etc/openldap/schema/cosine.schema
include     /etc/openldap/schema/inetorgperson.schema
include     /etc/openldap/schema/nis.schema
include         /etc/openldap/schema/ppolicy.schema
allow bind\_v2
pidfile     /var/run/openldap/slapd.pid
argsfile    /var/run/openldap/slapd.args
modulepath  /usr/lib64/openldap
moduleload  syncprov.la
loglevel acl conns trace sync stats args
database    bdb
suffix      "dc=fr-internal,dc=huawei,dc=com"
rootdn      "cn=Manager,dc=fr-internal,dc=huawei,dc=com"
rootpw      redhat
access to *
  by * read
directory   /var/lib/ldap
index objectClass                       eq,pres
index ou,cn,mail,surname,givenname      eq,pres,sub
index uidNumber,gidNumber,loginShell    eq,pres
index uid,memberUid                     eq,pres,sub
index nisMapName,nisMapEntry            eq,pres,sub
index entryCSN              eq
index entryUUID             eq
serverID 1
syncrepl      rid=001
              provider=ldap://10.206.98.109:389
              bindmethod=simple
              binddn="cn=Manager,dc=fr-internal,dc=huawei,dc=com"
              credentials=redhat
              searchbase="dc=fr-internal,dc=huawei,dc=com"
              attrs="*,+"
              schemachecking=off
              type=refreshAndPersist
              retry="60 +"
mirrormode on
overlay syncprov
syncprov-checkpoint 100 10
syncprov-sessionlog 100

EOF

ldap02 node

cat << EOF > /etc/openldap/slapd.conf
include     /etc/openldap/schema/core.schema
include     /etc/openldap/schema/cosine.schema
include     /etc/openldap/schema/inetorgperson.schema
include     /etc/openldap/schema/nis.schema
include         /etc/openldap/schema/ppolicy.schema
allow bind\_v2
pidfile     /var/run/openldap/slapd.pid
argsfile    /var/run/openldap/slapd.args
modulepath  /usr/lib64/openldap
moduleload  syncprov.la
loglevel acl conns trace sync stats args

database    bdb
suffix      "dc=fr-internal,dc=huawei,dc=com"
rootdn      "cn=Manager,dc=fr-internal,dc=huawei,dc=com"
rootpw      redhat
access to *
  by * read
directory   /var/lib/ldap
index objectClass                       eq,pres
index ou,cn,mail,surname,givenname      eq,pres,sub
index uidNumber,gidNumber,loginShell    eq,pres
index uid,memberUid                     eq,pres,sub
index nisMapName,nisMapEntry            eq,pres,sub
index entryCSN              eq
index entryUUID             eq
serverID 2
syncrepl      rid=001
              provider=ldap://10.206.98.72:389
              bindmethod=simple
              binddn="cn=Manager,dc=fr-internal,dc=huawei,dc=com"
              credentials=redhat
              searchbase="dc=fr-internal,dc=huawei,dc=com"
              attrs="*,+"
              schemachecking=off
              type=refreshAndPersist
              retry="60 +"
mirrormode on
overlay syncprov
syncprov-checkpoint 100 10
syncprov-sessionlog 100

EOF


rm -rf /etc/openldap/slapd.d/*

slaptest -f /etc/openldap/slapd.conf -F /etc/openldap/slapd.d

chown -R ldap.ldap /etc/openldap/slapd.d

systemctl restart slapd


cat << EOF > basedomain.ldif
# replace to your own domain name for "dc=***,dc=***" section

dn: dc=fr-internal,dc=huawei,dc=com
objectClass: top
objectClass: dcObject
objectclass: organization
o: Server Com
dc: fr-internal

dn: cn=Manager,dc=fr-internal,dc=huawei,dc=com
objectClass: organizationalRole
cn: Manager
description: Directory Manager

dn: ou=People,dc=fr-internal,dc=huawei,dc=com
objectClass: organizationalUnit
ou: People

dn: ou=Group,dc=fr-internal,dc=huawei,dc=com
objectClass: organizationalUnit
ou: Group

dn: ou=oseusers,dc=fr-internal,dc=huawei,dc=com
objectClass: organizationalUnit
ou: oseusers

EOF

ldapadd -x -D cn=Manager,dc=fr-internal,dc=huawei,dc=com -W -f basedomain.ldif 

ldapsearch -x -b "dc=fr-internal,dc=huawei,dc=com" -H ldap://10.206.98.72

cat << EOF > test01.ldif
dn: uid=test01,ou=oseusers,dc=fr-internal,dc=huawei,dc=com
uid: test01
cn: test01
objectClass: account
objectClass: posixAccount
objectClass: top
objectClass: shadowAccount
userPassword: 111111
shadowLastChange: 14323
shadowMax: 99999
shadowWarning: 7
loginShell: /bin/bash
uidNumber: 1005
gidNumber: 1005
homeDirectory: /home/test01
EOF

ldapadd -x -D "cn=Manager,dc=fr-internal,dc=huawei,dc=com" -W  -f test01.ldif

cat << EOF >ldap-user-Jane.ldif

dn: cn=Jane,ou=oseusers,dc=fr-internal,dc=huawei,dc=com
objectClass: person
objectClass: organizationalPerson
objectClass: inetOrgPerson
cn: Jane
sn: Smith
displayName: Jane Smith
mail: jane.smith@example.com
EOF

ldapadd -x -D "cn=Manager,dc=fr-internal,dc=huawei,dc=com" -W  -f ldap-user-Jane.ldif

ldapdelete -x -W -D "cn=Manager,dc=fr-internal,dc=huawei,dc=com" "uid=test01,ou=oseusers,dc=fr-internal,dc=huawei,dc=com" 

ldapsearch -x -b "dc=fr-internal,dc=huawei,dc=com" -H ldap://10.206.98.72
ldapsearch -x -b "dc=fr-internal,dc=huawei,dc=com" -H ldap://10.206.98.109


删除 LDAP 用户或组
删除用户：


    ldapdelete -x -W -D 'cn=Manager,dc=ho1ho,dc=com' "uid=ldapuser1,ou=People,dc=ho1ho,dc=com"  

删除组：


    ldapdelete -x -W -D 'cn=Manager,dc=ho1ho,dc=com' "cn=ldapuser1,ou=Group,dc=ho1ho,dc=com"  
	
host 环境初始化

移除/data01相关lv/vg

for i in `awk '{if($1~/^10./)print $2}' /etc/hosts`;do echo --$i--;ssh $i "grep data01  /etc/fstab";done

反注释/data01
for i in `awk '{if($1~/^10./)print $2}' /etc/hosts`;do echo --$i--;ssh $i 'sed -i  /data01/s/^/#/g /etc/fstab';done
移除相关lv/vg
for i in `awk '{if($1~/^10./)print $2}' /etc/hosts`;do echo --$i--;ssh $i 'umount /data01';done
for i in `awk '{if($1~/^10./)print $2}' /etc/hosts`;do echo --$i--;ssh $i "lvremove /dev/vg01/lv01";done
for i in `awk '{if($1~/^10./)print $2}' /etc/hosts`;do echo --$i--;ssh $i "vgremove vg01";done

master上面创建/var挂载点
for i in master01 master02 master03;do ssh $i "pvcreate /dev/xvde" ;done
for i in master01 master02 master03;do ssh $i "vgextend sysvg /dev/xvde" ;done

for i in master01 master02 master03;do ssh $i "lvresize -L +40G /dev/mapper/sysvg-lv_root -r" ;done
for i in master01 master02 master03;do ssh $i "df -h|grep root ";done

node host
for i in `awk '{if($1~/^10./ && $2 !~/master/)print $2}' /etc/hosts`;do echo --$i--;done


NFS Data mount 
pvcreate /dev/xvde
vgcreate datavg /dev/xvde
lvcreate -L +2040G -n datalv datavg
lvcreate -L +2040G -n datalv datavg
mkfs.ext4 /dev/mapper/datavg-datalv 
grep ocp /etc/fstab
/dev/mapper/datavg-datalv /ocp                ext4    defaults        0 0
mount -a

cat << EOF >  /etc/exportfs
/ocp/internal-registry *(rw,async,all_squash)
/ocp/internal-registry2 *(rw,async,all_squash)
/ocp/registry *(rw,async,no_root_squash)
/ocp/nfs1 *(rw,async,all_squash)
/ocp/nfs2 *(rw,async,all_squash)
/ocp/pv *(rw,async,no_root_squash)
EOF
chmod 777 /ocp/registry/
chown -R nfsnobody:nfsnobody /ocp/registry/

systemctl restart nfs-config
systemctl restart nfs-server
systemctl enable nfs-server
rpcinfo -p
 
[root@ldap01 ~]# showmount -e infra-svc01
Export list for infra-svc01:
/ocp/pv                 *
/ocp/nfs2               *
/ocp/nfs1               *
/ocp/registry           *
/ocp/internal-registry2 *
/ocp/internal-registry  *

mkdir /mnt/test
mount -t nfs infra-svc01:/ocp/pv /mnt/test
touch 1.txt

配置Yum
copy ocp package to /ocp/ose

yum install createrepo httpd -y
createrepo --worker=5 /ocp/ose
systemctl start httpd && systemctl enable httpd

cat << EOF > /etc/httpd/conf.d/ose3.7.conf 
Alias /ose "/ocp/ose"
<Directory "/ocp/ose">
  Options +Indexes +FollowSymLinks
Require all granted
</Directory>
<Location /ose>
SetHandler None
</Location>
EOF

cat << EOF > /etc/yum.repos.d/ose3.7.repo 
[openshift]
name=openshift 3.7
baseurl=http://yum.fr-internal.huawei.com/ose
gpgcheck=0
enabled=1
EOF

mkdir -p /etc/yum.repos.d/archive/
mv *.repo /etc/yum.repos.d/archive/

for i in `cat hosts`;do echo --$i--;scp /etc/yum.repos.d/ose3.7.repo $i:/etc/yum.repos.d/ose3.7.repo;done



配置docker-registry高可用
cat << EOF >> /etc/ansible/hosts
[reg-nodes]
infra-reg01.fr-internal.huawei.com
infra-reg02.fr-internal.huawei.com
EOF

ansible reg-nodes -a "uptime"
###on NFS server
chmod 777 /ocp/registry/
chown -R nfsnobody:nfsnobody /ocp/registry/

10.206.98.140:/ocp/registry  /var/lib/registry/    nfs    defaults        0 0

ansible reg-nodes -a "yum install -y docker-distribution"

ansible reg-nodes -a "systemctl start docker-distribution"
ansible reg-nodes -a "systemctl enable docker-distribution"

Openshift节点准备：

1. NTP 服务
2. SELINUX=enforcing
3. vi /etc/sysctl.conf
  net.ipv4.ip_forward=1
  sysctl -p
  sysctl -w net.ipv4.ip_forward=1

ansible nodes -a "sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config"
ansible nodes -a "sed -i 's/SELINUX=disabled/SELINUX=enforcing/g' /etc/selinux/config"

deploy dns
cat << EOF >>  /etc/resolv.conf 
# Generated by NetworkManager
search fr-internal.huawei.com huawei.com
nameserver 10.206.98.140 
nameserver 10.210.12.34
nameserver 10.210.12.35
nameserver 10.98.48.39
EOF

ansible nodes -m copy -a "src=/etc/resolv.conf dest=/etc/"


3.增加华为相关服务的/etc/hosts

10.1.133.33 saccws.huawei.com
10.2.77.58 sacc.huawei.com
ansible all -a 'sed -i "$a10.1.133.33 saccws.huawei.com" /etc/hosts'
ansible all -a 'sed -i "$a10.2.77.58 sacc.huawei.com" /etc/hosts'
ansible nodes -a 'sed -i "$a10.206.98.103 registry.fr-internal.huawei.com" /etc/hosts'

ansible nodes -a 'sed -i "$a10.3.23.46 ntp01-in.huawei.com" /etc/hosts'
ansible nodes -a 'sed -i "$a10.3.23.47 ntp03-in.huawei.com" /etc/hosts'
ansible nodes -a 'sed -i "$a10.98.51.95 ntp03-in.huawei.com" /etc/hosts'
for i in `awk '{if($1~/^10./)print $2}' /etc/hosts`;do echo --$i--;ssh $i "hostname -f"
fdisk -l|grep xvde
	
ansible nodes -m raw  -a "pvcreate /dev/xvdf" 
ansible nodes -m raw  -a "vgcreate dockervg /dev/xvdf" 
ansible nodes -a "yum install -y docker"

cat << EOF >> /etc/sysconfig/docker-storage-setup
VG=dockervg
EOF
docker-storage-setup

ansible nodes -m copy -a "src=/etc/sysconfig/docker-storage-setup dest=/etc/sysconfig/"
ansible nodes -a "docker-storage-setup"
ansible nodes -m raw -a "systemctl start docker && systemctl enable docker"
ansible nodes -a "docker version"
ansible nodes -m raw -a "docker info |grep -i space"

准备openshift安装
导入镜像：

docker load -i ocp3.7-image.tar
for i in `docker images |grep redhat.com|awk '{print $1":"$2}'`;do docker tag $i "registry.fr-internal.huawei.com:5000$(echo $i|awk -F 'com' '{print $2}')" ;done
for i in `docker images |grep redhat.com|awk '{print $1":"$2}'`;do docker rmi  $i ;done
for i in `docker images |grep v3.7.23-3|awk '{print $1":"$2}'`;do docker tag $i $(echo $i|awk -F ':' '{print $1":"$2":v3.7.23"}') ;done
for i in `docker images |grep v3.7.23-3|awk '{print $1":"$2}'`;do docker rmi  $i ;done

推镜像：

for i in `docker images |grep v3.7.23|awk '{print $1":"$2}'`;do echo $i ;done
for i in `docker images |grep v3.7.23|awk '{print $1":"$2}'`;do docker push $i ;done


配置ansible host inventory

cat << EOF >  /etc/ansible/hosts.bak

# Create an OSEv3 group that contains the master, nodes, etcd, and lb groups.
# The lb group lets Ansible configure HAProxy as the load balancing solution.
# Comment lb out if your load balancer is pre-configured.
[OSEv3:children]
masters
nodes
etcd
lb

# Set variables common for all OSEv3 hosts
[OSEv3:vars]
ansible_ssh_user=root
openshift_deployment_type=openshift-enterprise
openshift_release=v3.7
#openshift_release=v3.7.23
#openshift_disable_check=disk_availability,docker_image_availability,memory_availability,docker_storage
openshift_disable_check=disk_availability,docker_image_availability,memory_availability

# Enable cockpit
osm_use_cockpit=true
#
# Set cockpit plugins
osm_cockpit_plugins=['cockpit-kubernetes']
oreg_url=registry.fr-internal.huawei.com:5000/openshift3/ose-${component}:${version}
# Docker option
openshift_docker_options="--selinux-enabled --insecure-registry 172.30.0.0/16 --insecure-registry registry.fr-internal.huawei.com:5000 --log-driver json-file --log-opt max-size=100M --log-opt max-file=10"
openshift_docker_additional_registries=registry.fr-internal.huawei.com:5000
openshift_docker_insecure_registries=registry.fr-internal.huawei.com:5000
openshift_docker_blocked_registries=['public' , 'registry.access.redhat.com']
#Network plan
#openshift_portal_net=172.30.0.0/16
#osm_cluster_network_cidr=10.128.0.0/14
openshift_ocps_modify_imagestreams=true
# Uncomment the following to enable htpasswd authentication; defaults to
# DenyAllPasswordIdentityProvider.
openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]
openshift_hosted_registry_selector="infra=true"
#openshift_hosted_router_selector="router=true"

# Native high availbility cluster method with optional load balancer.
# If no lb group is defined installer assumes that a load balancer has
# been preconfigured. For installation the value of
# openshift_master_cluster_hostname must resolve to the load balancer
# or to one or all of the masters defined in the inventory if no load
# balancer is present.

openshift_master_cluster_method=native
openshift_master_cluster_hostname=openshift-api.fr-internal.huawei.com
openshift_master_cluster_public_hostname=openshift-cluster.fr-internal.huawei.com

# apply updated node defaults
openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['110'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}
openshift_master_default_subdomain=apps.mtl-expcloud.huawei.com
openshift_metrics_install_metrics=true
openshift_hosted_metrics_public_url=https://hawkular-metrics.apps.mtl-expcloud.huawei.com/hawkular/metrics
openshift_metrics_image_prefix=registry.fr-internal.huawei.com:5000/openshift3/
openshift_metrics_image_version=v3.7.42
openshift_logging_install_logging=true
openshift_logging_image_prefix=registry.fr-internal.huawei.com:5000/openshift3/
openshift_logging_image_version=v3.7.42
openshift_logging_es_nodeselector="logging=true"
openshift_logging_es_cluster_size=3
openshift_logging_es_number_of_shards=2
openshift_logging_es_number_of_replicas=1

# service catalog deployment
#openshift_enable_service_catalog=false
openshift_service_catalog_image_prefix=registry.fr-internal.huawei.com:5000/openshift3/ose-
ansible_service_broker_image_prefix=registry.fr-internal.huawei.com:5000/openshift3/ose-
ansible_service_broker_etcd_image_prefix=registry.fr-internal.huawei.com:5000/rhel7/
template_service_broker_prefix=registry.fr-internal.huawei.com:5000/openshift3/
openshift_service_catalog_image_version=v3.7.42
ansible_service_broker_image_tag=v3.7.42
#openshift_cockpit_deployer_prefix='registry.fr-internal.huawei.com:5000/openshift3/'
#openshift_cockpit_deployer_version='v3.7.42'
# override the default controller lease ttl
#osm_controller_lease_ttl=30
# enable ntp on masters to ensure proper failover
openshift_clock_enabled=true

[masters]
10.206.97.100  openshift_hostname=master01.fr-internal.huawei.com openshift_ip=10.206.97.100
10.206.96.164  openshift_hostname=master02.fr-internal.huawei.com openshift_ip=10.206.96.164
10.221.181.201 openshift_hostname=master03.fr-internal.huawei.com openshift_ip=10.221.181.201

# host group for etcd
[etcd]
10.206.97.100   openshift_ip=10.206.97.100
10.206.96.164   openshift_ip=10.206.96.164
10.221.181.201  openshift_ip=10.221.181.201

# Specify load balancer host
[lb]
10.206.98.130 openshift_hostname=api-lb01.fr-internal.huawei.com
10.206.98.139 openshift_hostname=api-lb02.fr-internal.huawei.com

# Specify nfs server host
[nfs-nodes]
infra-svc01.fr-internal.huawei.com

# host group for nodes, includes region info

[nodes]
10.206.97.100  openshift_hostname=master01.fr-internal.huawei.com openshift_ip=10.206.97.100 
10.206.96.164  openshift_hostname=master02.fr-internal.huawei.com openshift_ip=10.206.96.164
10.221.181.201 openshift_hostname=master03.fr-internal.huawei.com openshift_ip=10.221.181.201
10.221.177.75  openshift_hostname=node01.fr-internal.huawei.com  openshift_node_labels="{'region': 'primary', 'zone': 'default'}" openshift_ip=10.221.177.75
10.221.177.146 openshift_hostname=node02.fr-internal.huawei.com  openshift_node_labels="{'region': 'primary', 'zone': 'default'}" openshift_ip=10.221.177.146
10.206.99.164  openshift_hostname=node03.fr-internal.huawei.com  openshift_node_labels="{'region': 'primary', 'zone': 'default'}" openshift_ip=10.206.99.164
10.221.177.209 openshift_hostname=node04.fr-internal.huawei.com  openshift_node_labels="{'region': 'primary', 'zone': 'default'}" openshift_ip=10.221.177.209
10.221.177.216 openshift_hostname=infra-node01.fr-internal.huawei.com openshift_node_labels="{'region': 'infra', 'zone': 'default','infra':'true'}" openshift_ip=10.221.177.216
10.221.177.226 openshift_hostname=infra-node02.fr-internal.huawei.com openshift_node_labels="{'region': 'infra', 'zone': 'default','infra':'true'}" openshift_ip=10.221.177.226

[reg-nodes]
infra-reg01.fr-internal.huawei.com
infra-reg02.fr-internal.huawei.com

[ldap-nodes]
ldap01.fr-internal.huawei.com
ldap02.fr-internal.huawei.com

[lb-nodes]
infra-lb01.fr-internal.huawei.com
infra-lb02.fr-internal.huawei.com
EOF

ansible all -m copy -a "src=/etc/yum.repos.d/ose3.7.repo dest=/etc/yum.repos.d/"
ansible all -a "ls /etc/yum.repos.d/ose3.7.repo"

ansible nodes -a "yum install -y wget git net-tools bind-utils yum-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct \
  vim lrzsz python-setuptools"
ansible nodes -a "yum update -y"
ansible nodes -a "systemctl reboot"
ansible nodes -a "yum install -y atomic-openshift-excluder atomic-openshift-utils atomic-openshift-docker-excluder"

ansible nodes -a "atomic-openshift-excluder unexclude"

cp /etc/exports /etc/exports`date +%Y%m%d`

ansible-playbook  /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml

ansible-playbook  /usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml

配置serivce catalog
ansible-playbook /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/service-catalog.yml

添加firewall rule

ansible nodes -a 'sed -i "/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 39200 -j ACCEPT" /etc/sysconfig/iptables'
ansible lb -a 'sed -i  "/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 39200 -j ACCEPT" /etc/sysconfig/iptables'
ansible nodes -a "iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 39200 -j ACCEPT"
ansible lb  -a "iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 39200 -j ACCEPT"

ansible nodes -a 'sed -i "/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 3181 -j ACCEPT" /etc/sysconfig/iptables'
ansible lb -a 'sed -i "/.*--dport 22 -j ACCEPT.*/a\-A INPUT -p tcp -m state --state NEW -m tcp --dport 3181 -j ACCEPT" /etc/sysconfig/iptables'
ansible all  -a "iptables -A INPUT -p tcp -m state --state NEW -m tcp --dport 3181 -j ACCEPT"


创建asb-etc pv

cat << EOF >  asb-etcd-pv.yaml 
apiVersion: v1
kind: PersistentVolume
metadata:
  name: etcd-pv
spec:
  capacity:
    storage: 10Gi 
  accessModes:
  - ReadWriteOnce
  nfs: 
    path: /ocp/etcd
    server: 10.206.98.140
  persistentVolumeReclaimPolicy: Retain
EOF
oc create -f asb-etcd-pv.yaml -n openshift-ansible-service-broker

配置 dnsmasq
 
ansible nodes -a 'sed -i "$aaddress=/.apps.mtl-expcloud.huawei.com/10.206.98.209" /etc/dnsmasq.d/origin-dns.conf'
  
ansible nodes -a "systemctl restart dnsmasq"
 
创建用户
ansible masters -a "htpasswd -bc /etc/origin/master/htpasswd admin redhat"
oadm policy add-cluster-role-to-user cluster-admin admin

验证安装
ansible nodes -a "docker pull registry.fr-internal.huawei.com:5000/openshift3/ose-deployer:v3.7.42"
ansible nodes -a "docker tag registry.fr-internal.huawei.com:5000/openshift3/ose-deployer:v3.7.42 registry.access.redhat.com/openshift3/ose-deployer:v3.7.42"
ansible nodes -a "docker pull registry.fr-internal.huawei.com:5000/openshift3/ose:v3.7.42"
ansible nodes -a "docker tag registry.fr-internal.huawei.com:5000/openshift3/ose:v3.7.42 registry.access.redhat.com/openshift3/ose:v3.7.42"

oadm diagnostics


修改Huawei外部DNS对接监控
ansible nodes -a 'cp -rp /etc/NetworkManager/dispatcher.d/99-origin-dns.sh /root/'
scp master01:/etc/NetworkManager/dispatcher.d/99-origin-dns.sh /root/huawei/

修改99-origin-dns.sh从118行开始，并添加另外两行如下：

echo 'search cluster.local fr-internal.huawei.com huawei.com' >> ${NEW_RESOLV_CONF}
echo 'nameserver 10.210.12.34' >> ${NEW_RESOLV_CONF}
echo 'nameserver 10.210.12.35' >> ${NEW_RESOLV_CONF}

ansible nodes -m copy  -a 'src=/root/huawei/99-origin-dns.sh  dest=/etc/NetworkManager/dispatcher.d/99-origin-dns.sh'

ansible nodes -a 'systemctl restart NetworkManager'
ansible nodes -a 'cat /etc/resolv.conf'


配置Infra Haproxy 

ansible lb-nodes -a "yum install -y haproxy"

ansible lb-nodes -a "yum install -y keepalived"

ansible lb-nodes -a "cp  /etc/haproxy/haproxy.cfg /etc/haproxy/haproxy.cfg`date +%Y%m%d`"
ansible lb-nodes -a "cp  /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf`date +%Y%m%d`"

cat << EOF >  /etc/haproxy/haproxy.cfg 
global
    chroot      /var/lib/haproxy
    pidfile     /var/run/haproxy.pid
    maxconn     20000
    user        haproxy
    group       haproxy
    daemon
    log         /dev/log local0 info

    stats socket /var/lib/haproxy/stats

defaults
    mode                    http
    log                     global
    option                  httplog
    option                  dontlognull
    option forwardfor       except 127.0.0.0/8
    option                  redispatch
    retries                 3
    timeout http-request    10s
    timeout queue           1m
    timeout connect         10s
    timeout client          300s
    timeout server          300s
    timeout http-keep-alive 10s
    timeout check           10s
    maxconn                 20000

listen stats :9000
    mode http
    stats enable
    stats uri /
	
frontend http
    bind *:80
    default_backend http
    mode tcp
    option tcplog
frontend https
    bind *:443
    default_backend https
    mode tcp
    option tcplog

backend http
    balance roundrobin
    mode tcp
    server      infra-node01 10.221.177.216:80 check
    server      infra-node02 10.221.177.216:80 check
backend https
    balance roundrobin
    mode tcp
    server      infra-node01 10.221.177.216:443 check
    server      infra-node02 10.221.177.216:443 check
frontend registry
    bind *:5000
    default_backend registry
    mode tcp
    option tcplog
frontend ldap
    bind *:389
    default_backend ldap
    mode tcp
    option tcplog

backend registry
    balance source
    mode tcp
    server      infra-reg01 10.206.98.103:5000 check
    server      infra-reg02 10.206.98.106:5000 check
backend ldap
    balance source
    mode tcp
    server      ldap01 10.206.98.72:389 check
    server      ldap02 10.206.98.109:389 check
EOF


ansible lb-nodes -a "systemctl start haproxy"
ansible lb-nodes -a "systemctl enable haproxy"

配置Infra LB Keepalived主节点上

1.创建脚本

cat << EOF > /etc/keepalived/check_haproxy.sh
#!/bin/bash
#determine haproxy startup
if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
#if haproxy is top ,then start haproxy 
systemctl start haproxy
#sleep 3 secondes
sleep 1
    if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
    systemctl stop keepalived
    # if haproxy can't be started ,then stop haproxy ,finally , VIP will be moved to another server
    fi
fi
EOF
chmod +x /etc/keepalived/check_haproxy.sh


1.创建配置文件

cat << EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {                     
    router_id INFRALB
}

vrrp_script chk_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 1
    weight 2
}

vrrp_instance haproxy {
    state MASTER
    interface eth0
    virtual_router_id 110
    garp_master_delay 1
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123456
    }
    virtual_ipaddress {
        10.206.98.209/24 dev eth0
    }
    track_interface {
        eth0
    }
    track_script {
        chk_haproxy
    }
}
EOF


chmod +x /etc/keepalived/check_haproxy.sh

配置Infra LB Keepalived在从节点上

1.创建脚本

cat << EOF > /etc/keepalived/check_haproxy.sh
#!/bin/bash
#determine haproxy startup
if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
#if haproxy is top ,then start haproxy 
systemctl start haproxy
#sleep 3 secondes
sleep 1
    if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
    systemctl stop keepalived
    # if haproxy can't be started ,then stop haproxy ,finally , VIP will be moved to another server
    fi
fi
EOF
chmod +x /etc/keepalived/check_haproxy.sh


1.创建配置文件

cat << EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {                     
    router_id INFRALB
}

vrrp_script chk_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 1
    weight 2
}

vrrp_instance haproxy {
    state BACKUP
    interface eth0
    virtual_router_id 110
    garp_master_delay 1
    priority 20
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123456
    }
    virtual_ipaddress {
        10.206.98.209/24 dev eth0
    }
    track_interface {
        eth0
    }
    track_script {
        chk_haproxy
    }
}
EOF


ansible lb-nodes -a "systemctl start keepalived"

ansible lb-nodes -a "systemctl enable keepalived"


配置API LB Keepalived主节点上

ansible lb -a "yum install -y keepalived"
ansible lb -a "cp  /etc/keepalived/keepalived.conf /etc/keepalived/keepalived.conf`date +%Y%m%d`"

1.创建脚本

cat << EOF > /etc/keepalived/check_haproxy.sh
#!/bin/bash
#determine haproxy startup
if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
#if haproxy is top ,then start haproxy 
systemctl start haproxy
#sleep 3 secondes
sleep 1
    if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
    systemctl stop keepalived
    # if haproxy can't be started ,then stop haproxy ,finally , VIP will be moved to another server
    fi
fi
EOF
chmod +x /etc/keepalived/check_haproxy.sh


1.创建配置文件

cat << EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {                     
    router_id APILB
}

vrrp_script chk_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 1
    weight 2
}

vrrp_instance haproxy {
    state MASTER
    interface eth0
    virtual_router_id 120
    garp_master_delay 1
    priority 100
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123456
    }
    virtual_ipaddress {
        10.206.98.122/24 dev eth0
    }
    track_interface {
        eth0
    }
    track_script {
        chk_haproxy
    }
}
EOF


配置API LB Keepalived在从节点上

1.创建脚本

cat << EOF > /etc/keepalived/check_haproxy.sh
#!/bin/bash
#determine haproxy startup
if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
#if haproxy is top ,then start haproxy 
systemctl start haproxy
#sleep 3 secondes
sleep 1
    if [ \`ps -C haproxy --no-header |wc -l\` -eq 0 ] ; then
    systemctl stop keepalived
    # if haproxy can't be started ,then stop haproxy ,finally , VIP will be moved to another server
    fi
fi
EOF
chmod +x /etc/keepalived/check_haproxy.sh


1.创建配置文件

cat << EOF > /etc/keepalived/keepalived.conf
! Configuration File for keepalived
global_defs {                     
    router_id APILB
}

vrrp_script chk_haproxy {
    script "/etc/keepalived/check_haproxy.sh"
    interval 1
    weight 2
}

vrrp_instance haproxy {
    state BACKUP
    interface eth0
    virtual_router_id 120
    garp_master_delay 1
    priority 20
    advert_int 1
    authentication {
        auth_type PASS
        auth_pass 123456
    }
    virtual_ipaddress {
        10.206.98.122/24 dev eth0
    }
    track_interface {
        eth0
    }
    track_script {
        chk_haproxy
    }
}
EOF


ansible lb -a "systemctl start keepalived"

ansible lb -a "systemctl enable keepalived"


配置LDAP

备份master-config文件

ansible master -a "mkdir /root/ocp"
ansible masters -a "cp /etc/origin/master/master-config.yaml /root/ocp/master-config.yaml`date +%Y%m%d`" 
ansible masters -a  "ls -l /root/ocp/"

identityProviders:
  - challenge: true
    login: true
    mappingMethod: claim
    name: idm
    provider:
      apiVersion: v1
      kind: LDAPPasswordIdentityProvider
      attributes:
        email:
        - mail
        id:
        - dn
        name:
        - cn
        preferredUsername:
        - uid
      bindDN: cn=Manager,dc=fr-internal,dc=huawei,dc=com
      bindPassword: redhat
      insecure: true
      url: ldap://ldap.fr-internal.huawei.com/ou=oseusers,dc=fr-internal,dc=huawei,dc=com?cn

oadm policy add-cluster-role-to-user cluster-admin Jane
ansible masters -a "systemctl restart atomic-openshift-master-api"
	  
Metric数据持久化

配置EFK高可用集群 
 for i in node02.fr-internal.huawei.com node03.fr-internal.huawei.com node04.fr-internal.huawei.com ;do oc label nodes $i logging=true;done 
  
  vi /etc/ansible/hosts
   openshift_logging_es_cluster_size=3
   
   ansible-playbook  /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml
 
 2. Create mount directory
 
  for i in node02 node03 node04;do echo --$i--;ssh $i "fdisk -l |grep /dev/xvdg";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "pvcreate /dev/xvdg";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "vgcreate esdatavg /dev/xvdg";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "lvcreate -L 280G -n esdata-lv esdatavg";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "mkfs.xfs /dev/mapper/esdatavg-esdata--lv";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "mkdir -p /ocp/esdata";done
 
  for i in node02 node03 node04;do echo --$i--;ssh $i "echo '/dev/mapper/esdatavg-esdata--lv /ocp/esdata       xfs    defaults        1 2' >> /etc/fstab";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "mount -a";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "df -h |grep esdata";done
  for i in node02 node03 node04;do echo --$i--;ssh $i "chown 1000:1000 /ocp/esdata";done
  
 3. Deploy 3 nodes eslasticsearch cluster
  oc project logging
  oc adm policy add-scc-to-user privileged  \
       system:serviceaccount:logging:aggregated-logging-elasticsearch 
  
  for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc scale $dc --replicas=0
    oc patch $dc \
       -p '{"spec":{"template":{"spec":{"containers":[{"name":"elasticsearch","securityContext":{"privileged": true}}]}}}}'
  done
  
  for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc patch $dc \
        -p '{"spec":{"template":{"spec":{"nodeSelector":{"logging":"true"}}}}}'
  done
  
  for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    oc set volume $dc \
          --add --overwrite --name=elasticsearch-storage \
          --type=hostPath --path=/ocp/esdata
    oc rollout latest $dc
    oc scale $dc --replicas=1
  done

部署Gogs

oc new-project gogs

oc new-app -f gogs-persistent-template.yaml --param=HOSTNAME=gogs.apps.mtl-expcloud.huawei.com

--> Deploying template "gogs/gogs" for "gogs-persistent-template.yaml" to project gogs

     gogs
     ---------
     The Gogs git server (https://gogs.io/)

     * With parameters:
        * APPLICATION_NAME=gogs
        * HOSTNAME=gogs.apps.mtl-expcloud.huawei.com
        * GOGS_VOLUME_CAPACITY=1Gi
        * DB_VOLUME_CAPACITY=1Gi
        * Database Username=gogs
        * Database Password=gogs
        * Database Name=gogs
        * Database Admin Password=eBYhEpAE # generated
        * Maximum Database Connections=100
        * Shared Buffer Amount=12MB
        * Gogs Version=0.11.43
        * Installation lock=true
        * Skip TLS verification on webhooks=false

--> Creating resources ...
    serviceaccount "gogs" created
    service "gogs-postgresql" created
    deploymentconfig "gogs-postgresql" created
    service "gogs" created
    route "gogs" created
    deploymentconfig "gogs" created
    imagestream "gogs" created
    persistentvolumeclaim "gogs-data" created
    persistentvolumeclaim "gogs-postgres-data" created
    configmap "gogs-config" created
--> Success
    Access your application via route 'gogs.apps.mtl-expcloud.huawei.com' 

oc set volume dc/gogs --add --overwrite --name=gogs-config -m /opt/gogs/custom/conf/ -t configmap --configmap-name=gogs-config

for i in node01.fr-internal.huawei.com node02.fr-internal.huawei.com;do oc label nodes $i gogs=deploy;done
oc patch dc gogs \
        -p '{"spec":{"template":{"spec":{"nodeSelector":{"gogs":"deploy"}}}}}'
oc scale dc gogs --replicas=2

		
